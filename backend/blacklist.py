# -*- coding: utf-8 -*-
"""blacklist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qYPCVZsRE3SIThxl292iWiHnUJZRXCvt
"""

!apt-get update
!apt-get install tshark -y

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import subprocess
import pandas as pd

def run_analysis(pcap_file, blacklist_file, output_prefix="output"):
    # 1. Load blacklist
    blacklist = pd.read_csv(blacklist_file)

    if {"caller", "callee", "src_ip", "dst_ip"}.issubset(blacklist.columns):
        numbers = set(blacklist["caller"].astype(str)) | set(blacklist["callee"].astype(str))
        ips = set(blacklist["src_ip"].astype(str)) | set(blacklist["dst_ip"].astype(str))
    elif {"type", "value"}.issubset(blacklist.columns):
        numbers = set(blacklist.loc[blacklist["type"] == "number", "value"].astype(str))
        ips = set(blacklist.loc[blacklist["type"] == "ip", "value"].astype(str))
    else:
        raise ValueError("❌ Blacklist must have either (caller, callee, src_ip, dst_ip) OR (type, value) format")

    # 🧹 Clean blacklist values (strict)
    numbers = {x.strip() for x in numbers if x.strip() not in ["", "nan", "None"]}
    ips = {x.strip() for x in ips if x.strip() not in ["", "nan", "None"]}

    print(f"✅ Loaded blacklist: {len(numbers)} numbers, {len(ips)} IPs")
    print("📋 Numbers in blacklist:", numbers)
    print("📋 IPs in blacklist:", ips)

    # 2. Run tshark command
    tshark_command = [
        "tshark",
        "-r", pcap_file,
        "-Y", "sip",  # Filter for SIP packets
        "-T", "fields",
        "-e", "frame.number",
        "-e", "frame.time_epoch",
        "-e", "sip.from.user",
        "-e", "sip.to.user",
        "-e", "ip.src",
        "-e", "ip.dst",
        "-e", "sip.Call-ID",
        "-E", "header=y",
        "-E", "separator=,"
    ]

    try:
        result = subprocess.run(tshark_command, capture_output=True, text=True, check=True)
        tshark_output = result.stdout
    except subprocess.CalledProcessError as e:
        print(f"❌ Error running tshark: {e}")
        print(f"Stderr: {e.stderr}")
        return None, None # Return None for expected values on error

    # 3. Parse tshark output
    from io import StringIO
    df = pd.read_csv(StringIO(tshark_output))

    # Rename columns for clarity
    df.columns = ["frame_no", "timestamp", "caller", "callee", "src_ip", "dst_ip", "call_id"]

    # 4. Flag packets based on blacklist
    flagged = []
    for index, row in df.iterrows():
        reasons = []
        if str(row["caller"]) in numbers:
            reasons.append("caller_blacklisted")
        if str(row["callee"]) in numbers:
            reasons.append("callee_blacklisted")
        if str(row["src_ip"]) in ips:
            reasons.append("src_ip_blacklisted")
        if str(row["dst_ip"]) in ips:
            reasons.append("dst_ip_blacklisted")

        if reasons:
            row["reasons"] = ";".join(reasons)
            flagged.append(row)

    flagged_df = pd.DataFrame(flagged, columns=df.columns.tolist() + ["reasons"])

    # You can add more analysis steps here, like correlating flagged packets within a call

    # Return the original dataframe and the flagged packets dataframe
    return df, flagged_df

blacklist_df = generate_blacklist_from_pcap("new.pcap", "blacklist_new.csv")

# Step 1: Run analysis
df, flagged = run_analysis("full_prototype_test.pcap", "blacklist_new.csv", output_prefix="combined2")

# Step 2: Get and save summary
summarize_alerts(flagged, total_count=len(df), output_prefix="combined2")

import os

def summarize_alerts(flagged_df, total_count, output_prefix="output"):
    # Calculate summary statistics
    total_flagged_packets = len(flagged_df)
    percentage_flagged = (total_flagged_packets / total_count) * 100 if total_count > 0 else 0
    unique_flagged_calls = flagged_df["call_id"].nunique()

    # Save summary to a text file
    summary_filepath = f"{output_prefix}_summary.txt"
    with open(summary_filepath, "w") as f:
        f.write(f"--- Analysis Summary ---\n")
        f.write(f"Total packets analyzed: {total_count}\n")
        f.write(f"Total flagged packets: {total_flagged_packets}\n")
        f.write(f"Percentage of packets flagged: {percentage_flagged:.2f}%\n")
        f.write(f"Number of unique flagged calls: {unique_flagged_calls}\n")

        if not flagged_df.empty:
            f.write("\n--- Top 5 Flagged Call IDs ---\n")
            top_calls = flagged_df["call_id"].value_counts().head(5)
            for call_id, count in top_calls.items():
                f.write(f"{call_id}: {count} packets\n")

            f.write("\n--- Top 5 Flagging Reasons ---\n")
            # Flatten the list of reasons and count occurrences
            all_reasons = [reason for reasons_list in flagged_df["reasons"].str.split(';') for reason in reasons_list]
            if all_reasons:
                from collections import Counter
                reason_counts = Counter(all_reasons)
                top_reasons = reason_counts.most_common(5)
                for reason, count in top_reasons:
                    f.write(f"{reason}: {count} times\n")

    print(f"✅ Analysis summary saved to {summary_filepath}")

    # Save flagged dataframes to CSV
    flagged_csv_filepath = f"{output_prefix}_flagged.csv"
    flagged_df.to_csv(flagged_csv_filepath, index=False)
    print(f"✅ Flagged packets saved to {flagged_csv_filepath}")

    if not flagged_df.empty:
        flagged_calls_csv_filepath = f"{output_prefix}_flagged_calls.csv"
        flagged_df[["call_id"]].drop_duplicates().to_csv(flagged_calls_csv_filepath, index=False)
        print(f"✅ Unique flagged calls saved to {flagged_calls_csv_filepath}")

    # Remove the return statement
    # return summary