# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l_jhYru0BrF5Pzum54Tdt4FTrRWpI0Id
"""

!apt-get update
!apt-get install tshark -y

!apt-get install tshark -y

from google.colab import files
uploaded = files.upload()

import subprocess
import pandas as pd
import json
from datetime import datetime, timedelta
from collections import defaultdict
import nest_asyncio

# For Colab (avoids event loop issues with pyshark)
nest_asyncio.apply()

# ---------------- MOCK BLACKLIST ----------------
blacklist_numbers = {"999", "12345"}        # Example numbers
blacklist_ips = {"10.0.0.8", "192.168.1.50"} # Example IPs

# ---------------- Parse PCAP ----------------
def parse_pcap(filename):
    """
    Parse SIP call data from PCAP using tshark.
    Extracts: timestamp, caller, callee, src/dst IPs, duration
    """
    tshark_cmd = [
        "tshark", "-r", filename, "-Y", "sip",
        "-T", "fields",
        "-e", "frame.time_epoch",
        "-e", "sip.from.user",
        "-e", "sip.to.user",
        "-e", "ip.src",
        "-e", "ip.dst",
        "-e", "sip.Call-ID"
    ]

    result = subprocess.run(tshark_cmd, capture_output=True, text=True)
    lines = result.stdout.strip().split("\n")

    calls = []
    call_start = {}

    for line in lines:
        if not line.strip():
            continue
        parts = line.split("\t")
        if len(parts) < 6:
            continue

        ts, caller, callee, src_ip, dst_ip, call_id = parts
        ts = float(ts)
        timestamp = datetime.utcfromtimestamp(ts)

        # Detect call start (INVITE) and end (BYE)
        if call_id not in call_start:
            call_start[call_id] = (timestamp, caller, callee, src_ip, dst_ip)
        else:
            start_time, caller, callee, src_ip, dst_ip = call_start[call_id]
            duration = (timestamp - start_time).total_seconds()
            calls.append({
                "timestamp": start_time,
                "caller": caller,
                "callee": callee,
                "src_ip": src_ip,
                "dst_ip": dst_ip,
                "duration": duration
            })
            del call_start[call_id]

    return pd.DataFrame(calls)

# ---------------- Heuristic Checks ----------------
def analyze_calls(df):
    suspicious = []

    # Track call frequency
    call_counts = defaultdict(list)  # caller -> [timestamps]

    for idx, row in df.iterrows():
        flagged_reasons = []

        # 1. Blacklist check
        if row["caller"] in blacklist_numbers or row["callee"] in blacklist_numbers:
            flagged_reasons.append("Number in blacklist")
        if row["src_ip"] in blacklist_ips or row["dst_ip"] in blacklist_ips:
            flagged_reasons.append("IP in blacklist")

        # 2. Duration anomalies
        if row["duration"] < 5:
            flagged_reasons.append("Very short call (<5s)")
        if row["duration"] > 3600:
            flagged_reasons.append("Suspiciously long call (>1h)")

        # 3. High frequency (10 calls < 5 mins)
        caller = row["caller"]
        ts = row["timestamp"]
        call_counts[caller].append(ts)
        recent = [t for t in call_counts[caller] if ts - t <= timedelta(minutes=5)]
        if len(recent) >= 10:
            flagged_reasons.append("High call frequency (10+ calls in 5 min)")

        if flagged_reasons:
            suspicious.append({
                "timestamp": row["timestamp"].isoformat(),
                "caller": row["caller"],
                "callee": row["callee"],
                "src_ip": row["src_ip"],
                "dst_ip": row["dst_ip"],
                "duration": row["duration"],
                "reasons": flagged_reasons
            })

    return pd.DataFrame(suspicious)

# ---------------- MAIN ----------------
def run_analysis(pcap_file, output_prefix="suspects"):
    df = parse_pcap(pcap_file)
    print(f"Total calls parsed: {len(df)}")

    suspects = analyze_calls(df)
    print(f"Suspicious calls detected: {len(suspects)}")

    # Save results
    suspects.to_csv(f"{output_prefix}.csv", index=False)
    suspects.to_json(f"{output_prefix}.json", orient="records", indent=2)

    return suspects

suspects = run_analysis("SIP-Call-with-Proxy-Server (1).pcap")
suspects.head()

# Get unique Call-IDs or IP pairs to filter packets
suspect_ips = suspects[['src_ip', 'dst_ip']].drop_duplicates()
suspect_callers = suspects['caller'].unique()

import subprocess

def extract_sip_payloads(pcap_file, src_dst_list):
    """
    Extract SIP payloads for suspect calls.
    src_dst_list: list of tuples (src_ip, dst_ip) to filter
    """
    filters = " || ".join([f"(ip.src=={src} && ip.dst=={dst}) || (ip.src=={dst} && ip.dst=={src})"
                            for src, dst in src_dst_list])
    tshark_cmd = [
        "tshark", "-r", pcap_file, "-Y", f"sip && ({filters})",
        "-T", "fields",
        "-e", "frame.time_epoch",
        "-e", "ip.src",
        "-e", "ip.dst",
        "-e", "sip.Method",
        "-e", "text"
    ]

    result = subprocess.run(tshark_cmd, capture_output=True, text=True)
    lines = result.stdout.strip().split("\n")

    payloads = []
    for line in lines:
        parts = line.split("\t")
        if len(parts) < 5:
            continue
        ts, src, dst, method, text = parts
        payloads.append({
            "timestamp": float(ts),
            "src_ip": src,
            "dst_ip": dst,
            "method": method,
            "payload": text
        })
    return payloads

# Example usage:
src_dst_list = list(suspect_ips.itertuples(index=False, name=None))
sip_payloads = extract_sip_payloads("SIP-Call-with-Proxy-Server (1).pcap", src_dst_list)

for pkt in sip_payloads:
    if pkt['method'] not in ['INVITE', 'BYE', 'REGISTER', 'OPTIONS', 'ACK', 'CANCEL']:
        pkt['flag'] = "Unusual SIP method"

for pkt in sip_payloads:
    if len(pkt['payload']) > 2000:
        pkt['flag'] = "Abnormally large SIP message"

from collections import Counter
method_count = Counter([pkt['method'] for pkt in sip_payloads])
if method_count.get("OPTIONS", 0) > 50:  # threshold
    print("High OPTIONS request frequency detected")

dpi_df = pd.DataFrame(sip_payloads)
dpi_df.to_csv("suspect_payloads.csv", index=False)
dpi_df.to_json("suspect_payloads.json", orient="records", indent=2)

dpi_df.head()

dpi_df['timestamp'] = pd.to_datetime(dpi_df['timestamp'], unit='s')

dpi_df['flag'] = dpi_df['flag'].fillna("Normal")

dpi_df['flag'].value_counts()

dpi_df[dpi_df['flag'] != "Normal"]['src_ip'].value_counts()

import matplotlib.pyplot as plt

dpi_df['src_ip'].value_counts().plot(kind='bar')
plt.title("Top Source IPs in Suspicious Payloads")
plt.show()

dpi_df.set_index('timestamp')['flag'].resample('1min').count().plot()
plt.title("Suspicious Events Over Time")
plt.show()

import matplotlib.pyplot as plt

# Convert timestamp to datetime
# dpi_df['timestamp'] = pd.to_datetime(dpi_df['timestamp'], unit='s') # This line is no longer needed

# Set timestamp as index
# dpi_df = dpi_df.set_index('timestamp') # This line is no longer needed

# Filter only suspicious rows
suspicious_df = dpi_df[dpi_df['flag'] != "Normal"]

# Resample by minute (can change '1T' -> '5T' for 5 minutes, '1H' for 1 hour, etc.)
suspicious_counts = suspicious_df['flag'].resample('1T').count()

# Plot
plt.figure(figsize=(10,5))
suspicious_counts.plot(marker='o')
plt.title("Suspicious SIP Events Over Time")
plt.xlabel("Time")
plt.ylabel("Number of Suspicious Events")
plt.grid(True)
plt.show()

suspicious_calls = dpi_df[dpi_df['flag'] != "Normal"]
if not suspicious_calls.empty:
    print("‚ö†Ô∏è ALERT: Suspicious SIP traffic detected!")

!pip install dash plotly -q

from dash import Dash, dcc, html
import plotly.express as px
import pandas as pd

# Load your CSV (replace with your file path if needed)
df = pd.read_csv("suspect_payloads.csv")

# Filter suspicious only
suspicious = df[df['flag'].notna()]

# --- Figures ---
fig1 = px.bar(
    suspicious['src_ip'].value_counts().head(10),
    title="Top 10 Suspicious Source IPs",
    labels={"value": "Count", "index": "Source IP"}
)

fig2 = px.pie(
    suspicious,
    names='method',
    title="Distribution of Suspicious SIP Methods"
)

# --- Dashboard Layout ---
app = Dash(__name__)
app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),

    html.Div([
        html.H3("üìä Summary"),
        html.P(f"Total Packets: {len(df)}"),
        html.P(f"Suspicious Packets: {suspicious.shape[0]}"),
        html.P(f"Unique Source IPs: {df['src_ip'].nunique()}")
    ], style={"margin": "20px"}),

    html.H3("üö® Suspicious SIP Payloads"),
    dcc.Graph(figure=fig1),

    html.H3("üîç SIP Methods"),
    dcc.Graph(figure=fig2),

    html.H3("‚ö†Ô∏è Recent Alerts"),
    html.Ul([
        html.Li(f"[{row['timestamp']}] {row['src_ip']} ‚Üí {row['dst_ip']} | {row['method']} | {row['flag']}")
        for _, row in suspicious.tail(5).iterrows()
    ])
])

# The run_server call is removed as it was causing the error and requires ngrok setup.
# If you want to view the dashboard, you would typically need to use ngrok or a similar service
# to expose the Dash app running on the Colab instance.
# For now, the code defines the app but doesn't run the server.

# If you want to display the figures individually, you can use fig1.show() and fig2.show()

!pip install jupyter-dash plotly dash -q

import pandas as pd
from jupyter_dash import JupyterDash
from dash import dcc, html
import plotly.express as px

# Load your CSV (replace with your file path if needed)
df = pd.read_csv("suspect_payloads.csv")

# Filter suspicious only
suspicious = df[df['flag'].notna()]

# --- Figures ---
fig1 = px.bar(
    suspicious['src_ip'].value_counts().head(10),
    title="Top 10 Suspicious Source IPs",
    labels={"value": "Count", "index": "Source IP"}
)

fig2 = px.pie(
    suspicious,
    names='method',
    title="Distribution of Suspicious SIP Methods"
)

# --- Dashboard Layout ---
app = JupyterDash(__name__)
app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),

    html.Div([
        html.H3("üìä Summary"),
        html.P(f"Total Packets: {len(df)}"),
        html.P(f"Suspicious Packets: {suspicious.shape[0]}"),
        html.P(f"Unique Source IPs: {df['src_ip'].nunique()}")
    ], style={"margin": "20px"}),

    html.H3("üö® Suspicious SIP Payloads"),
    dcc.Graph(figure=fig1),

    html.H3("üîç SIP Methods"),
    dcc.Graph(figure=fig2),

    html.H3("‚ö†Ô∏è Recent Alerts"),
    html.Ul([
        html.Li(f"[{row['timestamp']}] {row['src_ip']} ‚Üí {row['dst_ip']} | {row['method']} | {row['flag']}")
        for _, row in suspicious.tail(5).iterrows()
    ])
])

# Run in external tab (safer in Colab)
app.run_server(mode="external")

!pip install dash dash-core-components dash-html-components jupyter-dash -q

import pandas as pd
import plotly.express as px
from dash import Dash, dcc, html

# Load your CSV
df = pd.read_csv("suspect_payloads.csv")
suspicious = df[df['flag'].notna()]

# --- Figures ---
fig1 = px.bar(
    suspicious['src_ip'].value_counts().head(10),
    title="Top 10 Suspicious Source IPs",
    labels={"value": "Count", "index": "Source IP"}
)

fig2 = px.pie(
    suspicious,
    names='method',
    title="Distribution of Suspicious SIP Methods"
)

# --- App Layout ---
app = Dash(__name__)
app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),

    html.Div([
        html.H3("üìä Summary"),
        html.P(f"Total Packets: {len(df)}"),
        html.P(f"Suspicious Packets: {suspicious.shape[0]}"),
        html.P(f"Unique Source IPs: {df['src_ip'].nunique()}")
    ]),

    html.H3("üö® Suspicious SIP Payloads"),
    dcc.Graph(figure=fig1),

    html.H3("üîç SIP Methods"),
    dcc.Graph(figure=fig2),

    html.H3("‚ö†Ô∏è Recent Alerts"),
    html.Ul([
        html.Li(f"[{row['timestamp']}] {row['src_ip']} ‚Üí {row['dst_ip']} | {row['method']} | {row['flag']}")
        for _, row in suspicious.tail(5).iterrows()
    ])
])

# --- Run the app on Colab ---
app.run(host="0.0.0.0", port=8050)

import pandas as pd
import plotly.express as px
from dash import Dash, dcc, html
from pyngrok import ngrok


df = pd.read_csv("suspect_payloads.csv")
suspicious = df[df['flag'].notna()]

# --- Figures ---
fig1 = px.bar(
    suspicious['src_ip'].value_counts().head(10),
    title="Top 10 Suspicious Source IPs",
    labels={"value": "Count", "index": "Source IP"}
)

fig2 = px.pie(
    suspicious,
    names='method',
    title="Distribution of Suspicious SIP Methods"
)

# --- App Layout ---
app = Dash(__name__)
app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),

    html.Div([
        html.H3("üìä Summary"),
        html.P(f"Total Packets: {len(df)}"),
        html.P(f"Suspicious Packets: {suspicious.shape[0]}"),
        html.P(f"Unique Source IPs: {df['src_ip'].nunique()}")
    ]),

    html.H3("üö® Suspicious SIP Payloads"),
    dcc.Graph(figure=fig1),

    html.H3("üîç SIP Methods"),
    dcc.Graph(figure=fig2),

    html.H3("‚ö†Ô∏è Recent Alerts"),
    html.Ul([
        html.Li(f"[{row['timestamp']}] {row['src_ip']} ‚Üí {row['dst_ip']} | {row['method']} | {row['flag']}")
        for _, row in suspicious.tail(5).iterrows()
    ])
])


public_url = ngrok.connect(8050)
print("üîó Dashboard URL:", public_url)


app.run(host="0.0.0.0", port=8050)

!pip install jupyter-dash -q

from dash import Dash, dcc, html
import plotly.express as px
import pandas as pd


app = Dash(__name__)

app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),
    dcc.Graph(figure=fig1),
    dcc.Graph(figure=fig2),
])


!pip install dash -q

import dash
from dash import Dash, html, dcc
import plotly.express as px
import pandas as pd

# Example data (replace with your VoIP data)
df = pd.DataFrame({
    "Time": ["10:00", "10:05", "10:10", "10:15"],
    "Call Volume": [5, 8, 2, 6],
    "Flags": [0, 1, 0, 2]
})

fig1 = px.line(df, x="Time", y="Call Volume", title="üìû Call Volume Over Time")
fig2 = px.bar(df, x="Time", y="Flags", title="üö® Flagged Calls")

# --- Dash app ---
app = Dash(__name__)
app.layout = html.Div([
    html.H1("üì° VoIP SIP Tracing Dashboard"),
    dcc.Graph(figure=fig1),
    dcc.Graph(figure=fig2),
])

app.run(host="0.0.0.0", port=8050, debug=False)

!pip install colab-dash -q

dpi_df.to_csv("suspect_payloads.csv", index=False)

dpi_df.to_json("suspect_payloads.json", orient="records", indent=2)

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

!ls /content/drive/MyDrive/

!ls /content/

!zip -r sample_data.zip /content/sample_data

from google.colab import files
files.download("sample_data.zip")
